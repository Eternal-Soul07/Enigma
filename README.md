# Enigma
Enigma, the AI Assistant:

The goal of the AI Assistant Model (Enigma) project is to create a sophisticated virtual assistant that can recognise gestures in addition to voices. It expands accessibility and user experience by proposing a dual-modality approach, which tackles the constraints of current virtual assistants. The project aims to accomplish reliable gesture recognition, intuitive control mapping, and precise hand pose estimate by combining a number of libraries and technologies, including Media Pipe, OpenCV, and PyAutoGUI.

Principal Accomplishments: Two-Modality Communication: Voice and gesture recognition features have been successfully integrated, enabling users to communicate with the system in both ways. Precise Hand Pose Estimation: Overcame obstacles such changes in lighting and hand postures to attain precise real-time recognition of hand landmarks. Strong Gesture Recognition: Specialised algorithms were created to differentiate between various hand gestures with the least amount of false positives or negatives. Enhancing user experience, intuitive control mapping maps known gestures to popular computer activities in a natural and simple way. Customizability: Increased adaptability and usability by allowing users to customise gesture mappings to suit their preferences. 

Capabilities Exhibited: Programming Skills: Exhibited mastery of Python programming and the use of a range of libraries and APIs to develop sophisticated features. Computer Vision: Using OpenCV and Media Pipe libraries, applied computer vision techniques are used for gesture recognition and hand tracking. Machine Learning: Showing comprehension and application of machine learning ideas, hand pose estimation was performed using pre-trained machine learning models. User Interface Design: To improve user involvement, intuitive user interfaces were created for visual feedback using hand landmarks and connections on the camera feed. Problem Solving: Through creative thinking and algorithm development, issues like reliable gesture recognition and accurate hand pose estimation were successfully addressed. Relevance to Target Position: There is a clear fit between this project and jobs that need experience with computer vision, artificial intelligence, machine learning, and human-computer interaction. For positions like AI engineer, computer vision engineer, or human-computer interface specialist, it demonstrates real-world experience in creating sophisticated software systems with real-time interaction capabilities. The initiative also emphasises creativity and problem-solving skills, which are critical for positions involving research and development in artificial intelligence and related domains.

The AI Assistant Model (Enigma) project has produced a working hand gesture detection system that can be used with voice instructions to give users a more flexible and intuitive virtual assistant experience. The project advances accessible and user-centric AI technologies by effectively implementing dual-modality interaction and overcoming obstacles in precise hand pose estimation and gesture detection. The system's adaptability to a wide range of user preferences and requirements is further enhanced by its customisable nature, hence augmenting its utility and effectiveness.
